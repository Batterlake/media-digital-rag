services:
  qdrant:
    image: qdrant/qdrant:latest
    restart: always
    ports:
      - 6333:6333
      - 6334:6334
    expose:
      - 6333
      - 6334
      - 6335
    configs:
      - source: qdrant_config
        target: /qdrant/config/production.yaml
    volumes:
      - ./qdrant_data:/qdrant/storage

  vllm:
    image: qwenllm/qwenvl:2-cu121
    restart: always
    shm_size: '50gb'
    entrypoint: "python -m vllm.entrypoints.openai.api_server --served-model-name Qwen2-VL-72B-Instruct-GPTQ-Int4 --model Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4 --gpu-memory-utilization 0.78 --max_model_len 14336  --limit-mm-per-prompt image=10"
    ports:
      - 6336:8000
    expose:
      - 8000
    volumes:
      - ./.cache/huggingface:/root/.cache/huggingface # persist huggingface cache
    env_file: "vllm.env"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['3']
              capabilities: [gpu]

  colpali:
    image: colpali-retriever
    build:
      context: retriever
      dockerfile: Dockerfile
    environment:
      batch_size: 8
    restart: always
    shm_size: '10gb'
    ports:
      - 6339:8000
    expose:
      - 8000
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface # persist huggingface cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['3']
              capabilities: [gpu]

configs:
  qdrant_config:
    content: |
      log_level: INFO
