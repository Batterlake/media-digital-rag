{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "pdfs = {\n",
    "    \"MALM\": \"https://www.ikea.com/us/en/assembly_instructions/malm-4-drawer-chest-white__AA-2398381-2-100.pdf\",\n",
    "    \"BILLY\": \"https://www.ikea.com/us/en/assembly_instructions/billy-bookcase-white__AA-1844854-6-2.pdf\",\n",
    "    \"BOAXEL\": \"https://www.ikea.com/us/en/assembly_instructions/boaxel-wall-upright-white__AA-2341341-2-100.pdf\",\n",
    "    \"ADILS\": \"https://www.ikea.com/us/en/assembly_instructions/adils-leg-white__AA-844478-6-2.pdf\",\n",
    "    \"MICKE\": \"https://www.ikea.com/us/en/assembly_instructions/micke-desk-white__AA-476626-10-100.pdf\",\n",
    "}\n",
    "\n",
    "output_dir = \"data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for name, url in pdfs.items():\n",
    "    response = requests.get(url)\n",
    "    pdf_path = os.path.join(output_dir, f\"{name}.pdf\")\n",
    "\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"Downloaded {name} to {pdf_path}\")\n",
    "\n",
    "print(\"Downloaded files:\", os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "\n",
    "def convert_pdfs_to_images(pdf_folder):\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "    all_images = {}\n",
    "\n",
    "    for doc_id, pdf_file in enumerate(pdf_files):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        images = convert_from_path(pdf_path)\n",
    "        all_images[doc_id] = images\n",
    "\n",
    "    return all_images\n",
    "\n",
    "\n",
    "all_images = convert_pdfs_to_images(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 8, figsize=(15, 10))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = all_images[0][i]\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from byaldi import RAGMultiModalModel\n",
    "\n",
    "docs_retrieval_model = RAGMultiModalModel.from_pretrained(\n",
    "    \"vidore/colpali-v1.2\", verbose=0\n",
    ")\n",
    "\n",
    "docs_retrieval_model.index(\n",
    "    input_path=\"data/\",\n",
    "    index_name=\"image_index\",\n",
    "    store_collection_with_index=False,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "text_query = \"How many people are needed to assemble the Malm?\"\n",
    "\n",
    "results = docs_retrieval_model.search(text_query, k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_images(results, all_images):\n",
    "    grouped_images = []\n",
    "\n",
    "    for result in results:\n",
    "        doc_id = result[\"doc_id\"]\n",
    "        page_num = result[\"page_num\"]\n",
    "        grouped_images.append(\n",
    "            all_images[doc_id][page_num - 1]\n",
    "        )  # page_num are 1-indexed, while doc_ids are 0-indexed. Source https://github.com/AnswerDotAI/byaldi?tab=readme-ov-file#searching\n",
    "\n",
    "    return grouped_images\n",
    "\n",
    "\n",
    "grouped_images = get_grouped_images(results, all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "\n",
    "min_pixels = 224 * 224\n",
    "max_pixels = 1024 * 1024\n",
    "\n",
    "vl_model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "vl_model.cuda().eval()\n",
    "\n",
    "vl_model_processor = Qwen2VLProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_multimodal_rag(\n",
    "    vl_model,\n",
    "    docs_retrieval_model,\n",
    "    vl_model_processor,\n",
    "    grouped_images,\n",
    "    text_query,\n",
    "    top_k,\n",
    "    max_new_tokens,\n",
    "):\n",
    "    results = docs_retrieval_model.search(text_query, k=top_k)\n",
    "    grouped_images = get_grouped_images(results, all_images)\n",
    "\n",
    "    chat_template = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"image\", \"image\": image} for image in grouped_images]\n",
    "            + [{\"type\": \"text\", \"text\": text_query}],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Prepare the inputs\n",
    "    text = vl_model_processor.apply_chat_template(\n",
    "        chat_template, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(chat_template)\n",
    "    inputs = vl_model_processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Generate text from the vl_model\n",
    "    generated_ids = vl_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # Decode the generated text\n",
    "    output_text = vl_model_processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = answer_with_multimodal_rag(\n",
    "    vl_model=vl_model,\n",
    "    docs_retrieval_model=docs_retrieval_model,\n",
    "    vl_model_processor=vl_model_processor,\n",
    "    grouped_images=grouped_images,\n",
    "    text_query=\"Как мне собрать стол Micke?\",\n",
    "    top_k=3,\n",
    "    max_new_tokens=500,\n",
    ")\n",
    "print(output_text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
